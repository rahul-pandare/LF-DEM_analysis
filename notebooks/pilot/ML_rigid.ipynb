{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da6df30-1c0d-428c-85a5-008f38283706",
   "metadata": {},
   "source": [
    "## machine learning model trial for predicting rigid particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a90f92-6bdf-495b-83e0-e834c9bc319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib               # type: ignore\n",
    "import numpy             as np  # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import matplotlib.colors as mcolors\n",
    "import platform\n",
    "from   pathlib           import Path\n",
    "import importlib\n",
    "import readFiles\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "importlib.reload(readFiles)\n",
    "\n",
    "# ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib rc parameters modification\n",
    "plt.rcParams.update({\n",
    "  \"figure.max_open_warning\" : 0,\n",
    "  \"text.usetex\"             : True,\n",
    "  \"text.latex.preamble\"     : r\"\\usepackage{amsmath, bm, type1cm}\",  # Added \\bm for bold math\n",
    "  \"figure.autolayout\"       : True,\n",
    "  \"font.family\"             : \"STIXGeneral\",\n",
    "  \"mathtext.fontset\"        : \"stix\",\n",
    "  \"font.size\"               : 8,\n",
    "  \"xtick.labelsize\"         : 8,\n",
    "  \"ytick.labelsize\"         : 8,\n",
    "  \"lines.linewidth\"         : 1,\n",
    "  \"lines.markersize\"        : 5,\n",
    "})\n",
    "#plt.rcParams['text.latex.preamble']= r\"\\usepackage{amsmath}\"\n",
    "matplotlib.rc('text', usetex=True)\n",
    "matplotlib.rcParams['text.latex.preamble'] = r'\\boldmath'\n",
    "\n",
    "colors = ['#4a91b5', '#e68139', '#5da258', '#87629b', '#1b9e77']\n",
    "\n",
    "\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    topDir = Path(\"/Volumes/rahul_2TB/high_bidispersity/new_data/\")\n",
    "    fig_save_path = Path(\"/Users/rahul/City College Dropbox/Rahul Pandare/CUNY/research/bidisperse_project/figures/ang_vel/\")\n",
    "elif platform.system() == 'Linux':\n",
    "    topDir = Path(\"/media/rahul/rahul_2TB/high_bidispersity/new_data/\")\n",
    "    fig_save_path = Path(\"/media/Linux_1TB/City College Dropbox/Rahul Pandare/CUNY/research/bidisperse_project/figures/ang_vel/\")\n",
    "else:\n",
    "    raise OSError(\"Unsupported OS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcbcf25-cc5a-4928-9adf-2763af411a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec03a0de-34a4-4634-bc4f-26f932bb0795",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input set\n",
    "\n",
    "npp  = 1000\n",
    "runs = 1\n",
    "phi  = 0.74\n",
    "vr   = '0.5'\n",
    "ar   = 1.4 #[1.0, 1.4, 2.0, 4.0]\n",
    "off  = 100\n",
    "k    = 7  # max number of neighbors\n",
    "\n",
    "phir     = f\"{phi:.3f}\" if phi != round(phi, 2) else f\"{phi:.2f}\"\n",
    "dataname = f\"{topDir}/NP_{npp}/phi_{phir}/ar_{ar}/Vr_{vr}\"\n",
    "\n",
    "frameArr = []\n",
    "angvel_all      = []\n",
    "neighAngVel_all = []\n",
    "rigid_all       = []\n",
    "radius_all      = []\n",
    "posx_all = []\n",
    "posy_all = []\n",
    "idxlen = []\n",
    "\n",
    "if os.path.exists(dataname):\n",
    "    for l in range(runs):\n",
    "        dataname  = f\"{topDir}/NP_{npp}/phi_{phir}/ar_{ar}/Vr_{vr}/run_{l+1}\"  \n",
    "        dataFile  = open(glob.glob(f'{dataname}/data_*.dat')[0], 'r')\n",
    "        parFile   = open(glob.glob(f'{dataname}/par_*.dat' )[0], 'r')\n",
    "        rigFile   = open(glob.glob(f'{dataname}/rig_*.dat' )[0], 'r')\n",
    "        datdata   = np.genfromtxt(dataFile)\n",
    "        pardata   = readFiles.readParFile(parFile)\n",
    "        rigdata   = readFiles.rigList(rigFile)\n",
    "        srate     = datdata[off:, 2]\n",
    "        totStrain = datdata[-1, 1]\n",
    "        parLines  = open(glob.glob(f'{dataname}/par_*.dat' )[0], 'r').readlines()\n",
    "\n",
    "        lx = float(parLines[3].split()[2]) \n",
    "        lz = float(parLines[5].split()[2])\n",
    "            \n",
    "        for i, frame in enumerate(pardata[off:]):\n",
    "            angvel = frame[:,8]\n",
    "            pidx   = frame[:, 0]\n",
    "            pr     = frame[:, 1]\n",
    "            px     = frame[:, 2]\n",
    "            pz     = frame[:, 3]\n",
    "            sr     = srate[i]            \n",
    "            \n",
    "            xmat, zmat = np.outer(px, np.ones(len(px))), np.outer(pz, np.ones(len(pz))) # broadcasting position array\n",
    "            dxij, dzij = xmat.transpose() - xmat, zmat.transpose() - zmat        # distance matrix\n",
    "            \n",
    "            # Lees Edwards boundary:\n",
    "            dxij[dzij > lz/2.]  -= sr*lx\n",
    "            dzij[dzij > lz/2.]  -= lz\n",
    "            \n",
    "            dxij[dzij < -lz/2.] += sr*lx\n",
    "            dzij[dzij < -lz/2.] += lz\n",
    "            \n",
    "            # X peridodic:\n",
    "            dxij[dxij >  lx/2.] -= lx\n",
    "            dxij[dxij < -lx/2.] += lx\n",
    "        \n",
    "            dij = np.sqrt(dxij**2 + dzij**2) # norm dist matrix\n",
    "            \n",
    "            for ii in pidx:\n",
    "                sorted_indices = np.argsort(dij[:, int(ii)]) # all neighbors sorted by distance to particle ii\n",
    "                within_cutoff  = dij[sorted_indices, int(ii)] <= 1.5 * (1 + float(ar)) # distance cuttoff\n",
    "                idx = sorted_indices[within_cutoff][1:k+1]  # skip self # this list wont crash\n",
    "                idxlen.append(len(idx))\n",
    "\n",
    "                neighAngVel = np.mean(angvel[idx])     # mean ang vel of nearest neighbors\n",
    "                neighAngVel_all.append(neighAngVel/sr) \n",
    "\n",
    "            rigList = [set(sum(rigFrame, [])) for rigFrame in rigdata]\n",
    "            rigid = np.array([0] * npp)\n",
    "            rigid[list(rigList[off+i])] = 1\n",
    "\n",
    "            angvel_all.extend(angvel/sr)\n",
    "            rigid_all.extend(list(rigid))\n",
    "            radius_all.extend(pr)\n",
    "            posx_all.extend(px)\n",
    "            posy_all.extend(pz)\n",
    "                \n",
    "angvel_all      = np.array(angvel_all) - np.mean(angvel_all)\n",
    "neighAngVel_all = np.array(neighAngVel_all) - np.mean(neighAngVel_all)\n",
    "rigid_all       = np.array(rigid_all)\n",
    "radius_all      = np.array(radius_all)\n",
    "posx_all        = np.array(posx_all)\n",
    "posy_all        = np.array(posy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2036048-5f80-4365-9f11-fb434fabc0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rigid = np.array([0] * npp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac729bfa-ca67-4246-8775-aac82db5c500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.2391, Val Loss=1.2360, Val Acc=0.1158, Precision=0.1141, Recall=0.9961\n",
      "Epoch 2: Train Loss=1.2358, Val Loss=1.2330, Val Acc=0.1271, Precision=0.1142, Recall=0.9822\n",
      "Epoch 3: Train Loss=1.2328, Val Loss=1.2302, Val Acc=0.1748, Precision=0.1145, Recall=0.9229\n",
      "Epoch 4: Train Loss=1.2301, Val Loss=1.2277, Val Acc=0.2792, Precision=0.1167, Recall=0.8075\n",
      "Epoch 5: Train Loss=1.2276, Val Loss=1.2254, Val Acc=0.3993, Precision=0.1233, Recall=0.6962\n",
      "Epoch 6: Train Loss=1.2253, Val Loss=1.2233, Val Acc=0.4853, Precision=0.1283, Recall=0.6041\n",
      "Epoch 7: Train Loss=1.2231, Val Loss=1.2214, Val Acc=0.5465, Precision=0.1332, Recall=0.5385\n",
      "Epoch 8: Train Loss=1.2212, Val Loss=1.2196, Val Acc=0.5915, Precision=0.1379, Recall=0.4900\n",
      "Epoch 9: Train Loss=1.2194, Val Loss=1.2179, Val Acc=0.6248, Precision=0.1427, Recall=0.4556\n",
      "Epoch 10: Train Loss=1.2177, Val Loss=1.2163, Val Acc=0.6481, Precision=0.1464, Recall=0.4300\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Stack features\n",
    "X = np.stack([angvel_all, neighAngVel_all, radius_all, posx_all, posy_all], axis=1)\n",
    "y = rigid_all\n",
    "\n",
    "# Use 1 million points\n",
    "subset_size = 1_000_000\n",
    "indices = np.random.choice(len(X), size=subset_size, replace=False)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val   = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Compute class weights for imbalance\n",
    "pos_weight_value = ((len(y_train) - y_train.sum()) / y_train.sum()).item()\n",
    "pos_weight = torch.tensor(pos_weight_value, dtype=torch.float32)\n",
    "\n",
    "# Define neural network (without sigmoid!)\n",
    "class RigidClusterNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RigidClusterNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(5, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  # no Sigmoid here!\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = RigidClusterNet()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(X_val)\n",
    "        val_output = torch.sigmoid(val_logits)  # apply sigmoid here!\n",
    "        val_loss = criterion(val_logits, y_val)\n",
    "        val_pred = (val_output > 0.5).float()\n",
    "        val_acc = (val_pred == y_val).float().mean()\n",
    "        tp = ((val_pred == 1) & (y_val == 1)).sum().item()\n",
    "        fp = ((val_pred == 1) & (y_val == 0)).sum().item()\n",
    "        fn = ((val_pred == 0) & (y_val == 1)).sum().item()\n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        recall = tp / (tp + fn + 1e-6)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={loss.item():.4f}, Val Loss={val_loss.item():.4f}, \"\n",
    "          f\"Val Acc={val_acc.item():.4f}, Precision={precision:.4f}, Recall={recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e4660a2-7d45-4610-ad4c-d8a626691b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=1.0793, Val Loss=1.0430, Acc=0.5678, Precision=0.1855, Recall=0.8214, F1=0.3026, AUC=0.7303\n",
      "Epoch 02: Train Loss=1.0556, Val Loss=1.0418, Acc=0.5646, Precision=0.1850, Recall=0.8258, F1=0.3023, AUC=0.7309\n",
      "Epoch 03: Train Loss=1.0531, Val Loss=1.0404, Acc=0.5773, Precision=0.1875, Recall=0.8108, F1=0.3046, AUC=0.7327\n",
      "Epoch 04: Train Loss=1.0514, Val Loss=1.0396, Acc=0.5702, Precision=0.1863, Recall=0.8205, F1=0.3036, AUC=0.7325\n",
      "Epoch 05: Train Loss=1.0497, Val Loss=1.0389, Acc=0.5658, Precision=0.1855, Recall=0.8266, F1=0.3030, AUC=0.7328\n",
      "Epoch 06: Train Loss=1.0490, Val Loss=1.0392, Acc=0.5643, Precision=0.1854, Recall=0.8292, F1=0.3030, AUC=0.7326\n",
      "Epoch 07: Train Loss=1.0476, Val Loss=1.0390, Acc=0.5649, Precision=0.1853, Recall=0.8272, F1=0.3028, AUC=0.7329\n",
      "Epoch 08: Train Loss=1.0474, Val Loss=1.0388, Acc=0.5635, Precision=0.1851, Recall=0.8298, F1=0.3027, AUC=0.7332\n",
      "Epoch 09: Train Loss=1.0471, Val Loss=1.0377, Acc=0.5633, Precision=0.1853, Recall=0.8314, F1=0.3030, AUC=0.7335\n",
      "Epoch 10: Train Loss=1.0464, Val Loss=1.0377, Acc=0.5616, Precision=0.1848, Recall=0.8326, F1=0.3025, AUC=0.7338\n",
      "Epoch 11: Train Loss=1.0457, Val Loss=1.0379, Acc=0.5537, Precision=0.1833, Recall=0.8418, F1=0.3011, AUC=0.7329\n",
      "Epoch 12: Train Loss=1.0457, Val Loss=1.0383, Acc=0.5575, Precision=0.1843, Recall=0.8396, F1=0.3023, AUC=0.7341\n",
      "Epoch 13: Train Loss=1.0458, Val Loss=1.0374, Acc=0.5540, Precision=0.1837, Recall=0.8441, F1=0.3018, AUC=0.7340\n",
      "Epoch 14: Train Loss=1.0445, Val Loss=1.0373, Acc=0.5640, Precision=0.1852, Recall=0.8291, F1=0.3028, AUC=0.7341\n",
      "Epoch 15: Train Loss=1.0449, Val Loss=1.0376, Acc=0.5622, Precision=0.1851, Recall=0.8330, F1=0.3029, AUC=0.7338\n",
      "Epoch 16: Train Loss=1.0448, Val Loss=1.0380, Acc=0.5510, Precision=0.1831, Recall=0.8471, F1=0.3011, AUC=0.7337\n",
      "Epoch 17: Train Loss=1.0446, Val Loss=1.0384, Acc=0.5516, Precision=0.1832, Recall=0.8464, F1=0.3012, AUC=0.7338\n",
      "Epoch 18: Train Loss=1.0445, Val Loss=1.0373, Acc=0.5525, Precision=0.1835, Recall=0.8458, F1=0.3015, AUC=0.7337\n",
      "Epoch 19: Train Loss=1.0435, Val Loss=1.0378, Acc=0.5584, Precision=0.1844, Recall=0.8380, F1=0.3023, AUC=0.7343\n",
      "Epoch 20: Train Loss=1.0436, Val Loss=1.0373, Acc=0.5564, Precision=0.1840, Recall=0.8397, F1=0.3019, AUC=0.7339\n",
      "Epoch 21: Train Loss=1.0438, Val Loss=1.0372, Acc=0.5491, Precision=0.1826, Recall=0.8478, F1=0.3004, AUC=0.7339\n",
      "Epoch 22: Train Loss=1.0438, Val Loss=1.0374, Acc=0.5520, Precision=0.1832, Recall=0.8448, F1=0.3010, AUC=0.7338\n",
      "Epoch 23: Train Loss=1.0437, Val Loss=1.0389, Acc=0.5559, Precision=0.1841, Recall=0.8414, F1=0.3020, AUC=0.7340\n",
      "Epoch 24: Train Loss=1.0430, Val Loss=1.0367, Acc=0.5554, Precision=0.1838, Recall=0.8408, F1=0.3016, AUC=0.7341\n",
      "Epoch 25: Train Loss=1.0429, Val Loss=1.0373, Acc=0.5572, Precision=0.1842, Recall=0.8393, F1=0.3021, AUC=0.7341\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Stack features\n",
    "X = np.stack([angvel_all, neighAngVel_all, radius_all, posx_all, posy_all], axis=1)\n",
    "y = rigid_all\n",
    "\n",
    "# Subset (1 million)\n",
    "subset_size = 1_000_000\n",
    "indices = np.random.choice(len(X), size=subset_size, replace=False)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Stratified train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val   = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Compute pos_weight for BCEWithLogitsLoss\n",
    "pos_weight_value = ((len(y_train) - y_train.sum()) / y_train.sum()).item()\n",
    "pos_weight = torch.tensor(pos_weight_value, dtype=torch.float32)\n",
    "\n",
    "# Define improved neural net\n",
    "class RigidClusterNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RigidClusterNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(5, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  # No sigmoid here\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Instantiate model\n",
    "model = RigidClusterNet()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# DataLoader for batch training\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "epochs = 25\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_X)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(X_val)\n",
    "        val_output = torch.sigmoid(val_logits)  # Apply sigmoid here\n",
    "        val_loss = criterion(val_logits, y_val)\n",
    "        val_pred = (val_output > 0.5).float()\n",
    "        val_acc = (val_pred == y_val).float().mean().item()\n",
    "\n",
    "        tp = ((val_pred == 1) & (y_val == 1)).sum().item()\n",
    "        fp = ((val_pred == 1) & (y_val == 0)).sum().item()\n",
    "        fn = ((val_pred == 0) & (y_val == 1)).sum().item()\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        recall = tp / (tp + fn + 1e-6)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "        auc = roc_auc_score(y_val.numpy(), val_output.numpy())\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}: Train Loss={train_loss:.4f}, Val Loss={val_loss.item():.4f}, \"\n",
    "          f\"Acc={val_acc:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, \"\n",
    "          f\"F1={f1:.4f}, AUC={auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3983f824-4791-4e41-8642-b5c1bd0816d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss=1.0703, Val Loss=1.0475, Acc=0.5606, Precision=0.1842, Recall=0.8315, F1=0.3016, AUC=0.7303\n",
      "Epoch 02: Train Loss=1.0535, Val Loss=1.0446, Acc=0.5642, Precision=0.1848, Recall=0.8268, F1=0.3021, AUC=0.7316\n",
      "Epoch 03: Train Loss=1.0499, Val Loss=1.0435, Acc=0.5661, Precision=0.1854, Recall=0.8257, F1=0.3028, AUC=0.7322\n",
      "Epoch 04: Train Loss=1.0480, Val Loss=1.0419, Acc=0.5650, Precision=0.1854, Recall=0.8285, F1=0.3030, AUC=0.7324\n",
      "Epoch 05: Train Loss=1.0479, Val Loss=1.0421, Acc=0.5517, Precision=0.1828, Recall=0.8436, F1=0.3004, AUC=0.7328\n",
      "Epoch 06: Train Loss=1.0461, Val Loss=1.0419, Acc=0.5596, Precision=0.1843, Recall=0.8346, F1=0.3019, AUC=0.7327\n",
      "Epoch 07: Train Loss=1.0454, Val Loss=1.0414, Acc=0.5596, Precision=0.1840, Recall=0.8324, F1=0.3014, AUC=0.7324\n",
      "Epoch 08: Train Loss=1.0453, Val Loss=1.0407, Acc=0.5614, Precision=0.1844, Recall=0.8306, F1=0.3017, AUC=0.7331\n",
      "Epoch 09: Train Loss=1.0445, Val Loss=1.0406, Acc=0.5579, Precision=0.1840, Recall=0.8372, F1=0.3018, AUC=0.7330\n",
      "Epoch 10: Train Loss=1.0442, Val Loss=1.0406, Acc=0.5513, Precision=0.1826, Recall=0.8437, F1=0.3002, AUC=0.7331\n",
      "Epoch 11: Train Loss=1.0441, Val Loss=1.0407, Acc=0.5501, Precision=0.1824, Recall=0.8455, F1=0.3001, AUC=0.7333\n",
      "Epoch 12: Train Loss=1.0442, Val Loss=1.0406, Acc=0.5664, Precision=0.1854, Recall=0.8252, F1=0.3028, AUC=0.7330\n",
      "Epoch 13: Train Loss=1.0435, Val Loss=1.0402, Acc=0.5542, Precision=0.1833, Recall=0.8417, F1=0.3011, AUC=0.7336\n",
      "Epoch 14: Train Loss=1.0430, Val Loss=1.0404, Acc=0.5624, Precision=0.1846, Recall=0.8301, F1=0.3021, AUC=0.7327\n",
      "Epoch 15: Train Loss=1.0431, Val Loss=1.0403, Acc=0.5541, Precision=0.1833, Recall=0.8414, F1=0.3010, AUC=0.7330\n",
      "Epoch 16: Train Loss=1.0426, Val Loss=1.0405, Acc=0.5556, Precision=0.1836, Recall=0.8398, F1=0.3013, AUC=0.7333\n",
      "Epoch 17: Train Loss=1.0432, Val Loss=1.0402, Acc=0.5518, Precision=0.1829, Recall=0.8447, F1=0.3008, AUC=0.7333\n",
      "Epoch 18: Train Loss=1.0420, Val Loss=1.0395, Acc=0.5601, Precision=0.1843, Recall=0.8334, F1=0.3018, AUC=0.7334\n",
      "Epoch 19: Train Loss=1.0417, Val Loss=1.0397, Acc=0.5548, Precision=0.1835, Recall=0.8413, F1=0.3013, AUC=0.7333\n",
      "Epoch 20: Train Loss=1.0413, Val Loss=1.0399, Acc=0.5586, Precision=0.1840, Recall=0.8349, F1=0.3015, AUC=0.7335\n",
      "Epoch 21: Train Loss=1.0414, Val Loss=1.0394, Acc=0.5604, Precision=0.1844, Recall=0.8331, F1=0.3019, AUC=0.7336\n",
      "Epoch 22: Train Loss=1.0418, Val Loss=1.0392, Acc=0.5557, Precision=0.1837, Recall=0.8408, F1=0.3016, AUC=0.7335\n",
      "Epoch 23: Train Loss=1.0414, Val Loss=1.0393, Acc=0.5552, Precision=0.1835, Recall=0.8399, F1=0.3011, AUC=0.7336\n",
      "Epoch 24: Train Loss=1.0413, Val Loss=1.0394, Acc=0.5560, Precision=0.1835, Recall=0.8385, F1=0.3011, AUC=0.7336\n",
      "Epoch 25: Train Loss=1.0411, Val Loss=1.0396, Acc=0.5580, Precision=0.1841, Recall=0.8377, F1=0.3019, AUC=0.7336\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# ========== Feature Engineering ==========\n",
    "angvel_diff = angvel_all - neighAngVel_all\n",
    "pos_magnitude = np.sqrt(posx_all**2 + posy_all**2)\n",
    "normalized_angvel = angvel_all / (radius_all + 1e-6)\n",
    "\n",
    "# Stack features (original + engineered)\n",
    "X = np.stack([\n",
    "    angvel_all,\n",
    "    neighAngVel_all,\n",
    "    radius_all,\n",
    "    posx_all,\n",
    "    posy_all,\n",
    "    angvel_diff,\n",
    "    pos_magnitude,\n",
    "    normalized_angvel\n",
    "], axis=1)\n",
    "y = rigid_all\n",
    "\n",
    "# ========== Subset and Normalize ==========\n",
    "subset_size = 1_000_000\n",
    "indices = np.random.choice(len(X), size=subset_size, replace=False)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Stratified train-val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val   = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Class imbalance weight\n",
    "pos_weight_value = ((len(y_train) - y_train.sum()) / y_train.sum()).item()\n",
    "pos_weight = torch.tensor(pos_weight_value, dtype=torch.float32)\n",
    "\n",
    "# ========== Define Neural Network ==========\n",
    "class RigidClusterNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RigidClusterNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(8, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 1)  # logits only\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Model setup\n",
    "model = RigidClusterNet()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# DataLoader for batch training\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=2048, shuffle=True)\n",
    "\n",
    "# ========== Training Loop ==========\n",
    "epochs = 25\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_X)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(X_val)\n",
    "        val_output = torch.sigmoid(val_logits)\n",
    "        val_loss = criterion(val_logits, y_val)\n",
    "        val_pred = (val_output > 0.5).float()\n",
    "        val_acc = (val_pred == y_val).float().mean().item()\n",
    "\n",
    "        tp = ((val_pred == 1) & (y_val == 1)).sum().item()\n",
    "        fp = ((val_pred == 1) & (y_val == 0)).sum().item()\n",
    "        fn = ((val_pred == 0) & (y_val == 1)).sum().item()\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        recall = tp / (tp + fn + 1e-6)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "        auc = roc_auc_score(y_val.numpy(), val_output.numpy())\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}: Train Loss={train_loss:.4f}, Val Loss={val_loss.item():.4f}, \"\n",
    "          f\"Acc={val_acc:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, \"\n",
    "          f\"F1={f1:.4f}, AUC={auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
